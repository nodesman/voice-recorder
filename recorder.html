<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Audio Recorder Component (Corrected)</title>
    <style>
        /* --- General Styles --- */
        body {
            background-color: #222;
            color: #eee;
            font-family: sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
        }
        .audio-recorder {
            width: 100%;
            max-width: 400px; /* Or adjust as needed */
            height: 50px; /* Height of the main bar */
            position: relative; /* Crucial for absolute positioning children */
            box-sizing: border-box;
        }

        /* --- State Containers --- */
        /* Base styles for the pill bar shape and content alignment */
        .recorder-state {
            background-color: #444;
            border-radius: 25px; /* Pill shape */
            height: 100%;
            width: 100%;
            box-sizing: border-box;
            display: flex; /* Use flex for content alignment */
            align-items: center;
            justify-content: space-between;
            padding: 5px 10px;
        }

        /* --- Visibility Control --- */
        /* Hide all specific state containers by default */
        .idle-state,
        .recording-state,
        .processing-view { /* Note: .processing-view is the wrapper now */
            display: none;
            width: 100%; /* Ensure hidden elements don't affect layout if needed */
            height: 100%;
        }

        /* Show the ACTIVE state based on parent data-state */
        .audio-recorder[data-state="idle"] .idle-state {
            display: flex;
        }
        .audio-recorder[data-state="recording"] .recording-state {
            display: flex;
        }
        .audio-recorder[data-state="processing"] .processing-view {
            display: block; /* Show the wrapper for processing */
            height: auto; /* Allow wrapper to adapt if text grows */
        }

        /* --- Idle State --- */
        .idle-state { /* Inherits .recorder-state flex properties */
            justify-content: flex-end;
            padding: 5px 15px;
        }
        .mic-icon-svg {
            width: 28px;
            height: 28px;
            fill: #AAAAAA;
            cursor: pointer;
            transition: fill 0.2s ease;
        }
        .mic-icon-svg:hover {
            fill: #CCCCCC;
        }

        /* --- Recording State --- */
        /* Styles for elements inside .recording-state */
        .recording-state .waveform-canvas {
            flex-grow: 1;
            height: 40px; /* Height within the bar */
            margin: 0 10px;
        }
        .recording-state .timer {
            color: white;
            font-size: 0.9em;
            margin-left: -35px; /* Position timer over the canvas end */
            margin-right: 15px;
            z-index: 1;
            background-color: #444444cc;
            padding: 1px 3px;
            border-radius: 3px;
        }
        .recording-state .icon-button {
            width: 40px;
            height: 40px;
            flex-shrink: 0;
            cursor: pointer;
        }
        .recording-state .icon-button circle {
             transition: fill 0.2s ease;
        }
         .recording-state .cancel-button:hover circle {
             fill: #666; /* Darker gray for cancel hover */
         }
         .recording-state .confirm-button:hover circle {
             fill: #eee; /* Lighter gray for confirm hover */
         }

        /* --- Processing State View --- */
         .processing-view {
             /* This container holds the text and the bar */
             position: relative; /* Allows absolute positioning of text relative to this */
             width: 100%;
             /* height is determined by the processing-bar inside */
         }
        .transcription-text {
            color: #aaa;
            font-size: 0.9em;
            text-align: center;
            white-space: nowrap;
            /* Position above the main component area */
            position: absolute;
            bottom: calc(100% + 5px); /* Place it 5px above the component's top */
            left: 50%;
            transform: translateX(-50%);
            width: auto; /* Allow text to determine width */
            background-color: #222; /* Match body background */
            padding: 0 5px;
        }
         /* This is the actual pill bar for processing state */
         .processing-bar {
            /* Inherits .recorder-state styles (background, border-radius, height, flex, etc.) */
             width: 100%; /* Ensure it fills the .processing-view horizontal space */
         }
        /* Styles for elements inside .processing-bar */
        .processing-bar .waveform-canvas {
            flex-grow: 1;
            height: 40px;
            margin: 0 10px;
        }
        .processing-bar .total-duration {
             color: white;
             font-size: 0.9em;
             margin-left: 5px;
             margin-right: 5px;
             flex-shrink: 0;
        }
        .processing-bar .spinner-svg {
            width: 24px;
            height: 24px;
            animation: spin 1.5s linear infinite;
            flex-shrink: 0;
        }
        @keyframes spin {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

    </style>
</head>
<body>

    <div class="audio-recorder" data-state="idle">

        <div class="recorder-state idle-state">
            <svg class="mic-icon-svg" id="micButton" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
              <path d="M12,14c1.66,0,3-1.34,3-3V5c0-1.66-1.34-3-3-3S9,3.34,9,5v6C9,12.66,10.34,14,12,14z M17,11c0,2.76-2.24,5-5,5 s-5-2.24-5-5H5c0,3.53,2.61,6.43,6,6.92V21h2v-3.08c3.39-0.49,6-3.39,6-6.92H17z"/>
            </svg>
        </div>

        <div class="recorder-state recording-state">
            <svg class="icon-button cancel-button" id="cancelButton" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
              <circle cx="50" cy="50" r="48" fill="#555555"/>
              <line x1="30" y1="30" x2="70" y2="70" stroke="white" stroke-width="10" stroke-linecap="round"/>
              <line x1="70" y1="30" x2="30" y2="70" stroke="white" stroke-width="10" stroke-linecap="round"/>
            </svg>
            <canvas class="waveform-canvas" id="recordingWaveformCanvas" width="400" height="80"></canvas>
            <span class="timer" id="timerDisplay">0:00</span>
            <svg class="icon-button confirm-button" id="confirmButton" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
              <circle cx="50" cy="50" r="48" fill="white"/>
              <polyline points="30,50 45,65 70,35" fill="none" stroke="#333333" stroke-width="10" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
        </div>

        <div class="processing-view"> <div class="transcription-text" id="transcriptionDisplay">Processing...</div>
             <div class="recorder-state processing-bar">
                 <canvas class="waveform-canvas" id="processingWaveformCanvas" width="400" height="80"></canvas>
                 <span class="total-duration" id="totalDurationDisplay">0:00</span>
                 <svg class="spinner-svg" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                   <g transform="translate(50 50)">
                     <line x1="0" y1="-40" x2="0" y2="40" stroke="white" stroke-width="12" stroke-linecap="round"/>
                     <line x1="-40" y1="0" x2="40" y2="0" stroke="white" stroke-width="12" stroke-linecap="round"/>
                     <line x1="-28.3" y1="-28.3" x2="28.3" y2="28.3" stroke="white" stroke-width="12" stroke-linecap="round"/>
                     <line x1="-28.3" y1="28.3" x2="28.3" y2="-28.3" stroke="white" stroke-width="12" stroke-linecap="round"/>
                   </g>
                 </svg>
            </div>
        </div>

    </div>

    <script>
        const AudioRecorder = (() => {
            // --- DOM Elements ---
            let recorderContainer;
            let micButton;
            let cancelButton;
            let confirmButton;
            let recordingCanvas;
            let processingCanvas;
            let timerDisplay;
            let totalDurationDisplay;
            let transcriptionDisplay;

            // --- Audio & State Variables ---
            let isRecording = false;
            let mediaStream = null;
            let mediaRecorder = null;
            let audioContext = null;
            let analyserNode = null;
            let sourceNode = null;
            let recordedChunks = [];
            let recordingStartTime;
            let timerIntervalId = null;
            let animationFrameId = null;

            const WAVEFORM_BAR_COUNT = 60; // How many bars to show
            let waveformHistory = new Array(WAVEFORM_BAR_COUNT).fill(0);

            // --- Initialization ---
            function init() {
                recorderContainer = document.querySelector('.audio-recorder');
                micButton = document.getElementById('micButton');
                cancelButton = document.getElementById('cancelButton');
                confirmButton = document.getElementById('confirmButton');
                recordingCanvas = document.getElementById('recordingWaveformCanvas');
                // Select processing canvas within its specific bar container now
                processingCanvas = document.querySelector('.processing-bar .waveform-canvas');
                timerDisplay = document.getElementById('timerDisplay');
                totalDurationDisplay = document.getElementById('totalDurationDisplay');
                transcriptionDisplay = document.getElementById('transcriptionDisplay');

                if (!recorderContainer || !micButton || !cancelButton || !confirmButton || !recordingCanvas || !processingCanvas) {
                    console.error("Recorder UI elements not found! Check IDs and structure.");
                    return;
                }

                micButton.addEventListener('click', startRecording);
                cancelButton.addEventListener('click', () => stopRecording(false));
                confirmButton.addEventListener('click', () => stopRecording(true));

                console.log("Audio Recorder Initialized");
                 // Ensure initial state is visually correct based on HTML default
                setState(recorderContainer.dataset.state || 'idle');
            }

            // --- State Management ---
            function setState(newState) {
                 // Only update state if the container exists
                if (recorderContainer) {
                    recorderContainer.dataset.state = newState;
                }
                isRecording = (newState === 'recording');
                console.log("State changed to:", newState);
                 // The CSS now handles showing/hiding based on data-state
            }

            // --- Recording Logic ---
            async function startRecording() {
                if (isRecording) return;
                console.log("Attempting to start recording...");

                try {
                    // --- Request microphone access ---
                    mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    console.log("Microphone access granted.");

                    // --- Reset state variables ---
                    waveformHistory.fill(0);
                    recordedChunks = [];
                    if(timerIntervalId) clearInterval(timerIntervalId); // Clear any previous timer
                    if(animationFrameId) cancelAnimationFrame(animationFrameId); // Clear any previous animation

                    // --- Setup Web Audio API for Visualization ---
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    sourceNode = audioContext.createMediaStreamSource(mediaStream);
                    analyserNode = audioContext.createAnalyser();
                    analyserNode.fftSize = 256;
                    analyserNode.smoothingTimeConstant = 0.5;
                    sourceNode.connect(analyserNode);

                    // --- Setup MediaRecorder ---
                    const options = { mimeType: 'audio/webm;codecs=opus' };
                    try {
                         mediaRecorder = new MediaRecorder(mediaStream, options);
                    } catch(err) {
                        console.warn("opus/webm not supported, trying default", err);
                        mediaRecorder = new MediaRecorder(mediaStream);
                    }
                    mediaRecorder.ondataavailable = handleDataAvailable;
                    mediaRecorder.onstop = handleStop; // handleStop now manages the 'shouldSave' logic internally

                    // --- Start recording process ---
                    mediaRecorder.start(100); // Start collecting data, optionally specify timeslice in ms
                    recordingStartTime = Date.now();
                    setState('recording');
                    startTimer();
                    visualize(); // Start animation loop
                    
                    console.log("Recording started. State:", mediaRecorder.state);

                } catch (err) {
                    console.error("Error starting recording:", err);
                    alert(`Could not start recording: ${err.message}`);
                    cleanUpAudio(); // Clean up any partial setup
                    setState('idle'); // Ensure UI returns to idle
                }
            }

            function stopRecording(shouldSaveAndProcess) {
                console.log(`Stopping recording. Save: ${shouldSaveAndProcess}`);
                 if (!mediaRecorder || mediaRecorder.state === 'inactive') {
                     console.log("Recorder not active or already stopped.");
                     // Ensure UI is idle if something went wrong
                      if(recorderContainer && recorderContainer.dataset.state !== 'idle') {
                         setState('idle');
                         cleanUpAudio();
                         stopTimer();
                         stopVisualization();
                     }
                     return;
                 }

                // Store the intent - the actual processing happens in onstop
                mediaRecorder.shouldSaveAndProcess = shouldSaveAndProcess;
                mediaRecorder.stop();
                // Stop visualization and timer immediately for responsiveness
                stopVisualization();
                stopTimer();
            }

            function handleDataAvailable(event) {
                if (event.data.size > 0) {
                    recordedChunks.push(event.data);
                }
            }

            // This now receives the flag implicitly via the recorder instance
            function handleStop() {
                 console.log("MediaRecorder stopped.");
                 // Use the flag we attached before calling stop()
                 const shouldSaveAndProcess = !!mediaRecorder?.shouldSaveAndProcess;
                 const finalDuration = recordingStartTime ? Math.floor((Date.now() - recordingStartTime) / 1000) : 0;

                 if (shouldSaveAndProcess && recordedChunks.length > 0) {
                    setState('processing');
                    totalDurationDisplay.textContent = formatTime(finalDuration);
                    transcriptionDisplay.textContent = "Processing..."; // Reset text

                    // Draw final static waveform on the processing canvas
                    drawWaveform(processingCanvas, waveformHistory);

                    // --- Create Blob and Simulate Processing ---
                    const finalBlob = new Blob(recordedChunks, { type: mediaRecorder.mimeType || 'audio/webm' });
                    console.log("Final Blob created:", finalBlob.size, finalBlob.type);

                    // Placeholder for actual processing (e.g., send to Electron main process)
                    setTimeout(() => {
                        // Ensure we are still in processing state before updating text
                         if (recorderContainer && recorderContainer.dataset.state === 'processing') {
                             transcriptionDisplay.textContent = "Placeholder transcription result.";
                             console.log("Processing 'complete'. In real app, handle blob.");
                             // Optional: revert to idle after showing result?
                             // setTimeout(() => setState('idle'), 4000);
                         }
                    }, 2000); // Simulate 2 seconds processing

                 } else {
                     console.log("Recording cancelled or no data.");
                     setState('idle'); // Go back to idle if cancelled or no data
                 }

                 // --- Final Cleanup ---
                 cleanUpAudio();
                 recordedChunks = []; // Clear chunks for next time
                 // Clear the flag
                 if(mediaRecorder) delete mediaRecorder.shouldSaveAndProcess;
            }

            // --- Cleanup ---
            function cleanUpAudio() {
                console.log("Cleaning up audio resources.");
                 // Stop tracks first
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                    mediaStream = null;
                }
                // Disconnect Web Audio nodes
                if (sourceNode) {
                    sourceNode.disconnect();
                    sourceNode = null;
                }
                // Close AudioContext
                if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close().catch(e => console.warn("Error closing AudioContext:", e));
                    audioContext = null;
                }
                // Release MediaRecorder reference
                mediaRecorder = null;
                analyserNode = null;
            }


            // --- Visualization ---
            function visualize() {
                // Add this check at the beginning
                if (!analyserNode || recorderContainer?.dataset.state !== 'recording') {
                     console.log("Visualizer stopping: No analyser or not recording."); // Log why it might stop
                     stopVisualization();
                     return;
                }

                const bufferLength = analyserNode.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                analyserNode.getByteFrequencyData(dataArray); // Or getByteTimeDomainData

                // Calculate simple average amplitude
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += dataArray[i];
                }
                const average = sum / bufferLength;

                // Normalize (0-255 -> 0.0-1.0) - Adjust scaling factor as needed
                const normalizedAmplitude = Math.min(1.0, (average / 128.0) * 1.2); // Scale factor can be tweaked

                // --- ADD LOGS HERE ---
                console.log(`Avg: ${average.toFixed(2)}, Norm: ${normalizedAmplitude.toFixed(2)}`);
                // Optional: Log the raw data occasionally to see its range
                // if (Math.random() < 0.1) console.log(dataArray); // Log raw data sometimes
                // --- END LOGS ---

                // Update history buffer (FIFO)
                waveformHistory.push(normalizedAmplitude);
                if (waveformHistory.length > WAVEFORM_BAR_COUNT) {
                    waveformHistory.shift();
                }

                // Draw using the history
                drawWaveform(recordingCanvas, waveformHistory);

                // Continue the loop
                animationFrameId = requestAnimationFrame(visualize);
            }

             function stopVisualization() {
                if (animationFrameId) {
                    cancelAnimationFrame(animationFrameId);
                    animationFrameId = null;
                   // console.log("Visualization stopped."); // Can be noisy
                }
                // Optionally clear the recording canvas when visualization stops
                 if(recordingCanvas) {
                    const ctx = recordingCanvas.getContext('2d');
                    ctx.clearRect(0, 0, recordingCanvas.width, recordingCanvas.height);
                 }
            }

            // Generic draw function
            function drawWaveform(canvas, historyData) {
                 // Ensure canvas and context are valid
                 if (!canvas || typeof canvas.getContext !== 'function') return;
                 const ctx = canvas.getContext('2d');
                 if (!ctx) return;

                 const width = canvas.width;
                 const height = canvas.height;
                 const centerY = height / 2;
                 const numBars = historyData.length;
                 // Ensure numBars is positive to avoid division by zero
                 const barWidth = numBars > 0 ? width / numBars : width;

                 ctx.clearRect(0, 0, width, height);
                 ctx.strokeStyle = 'white';
                 ctx.lineWidth = Math.max(1, barWidth * 0.6);
                 ctx.lineCap = 'round';
                 ctx.beginPath();

                 for (let i = 0; i < numBars; i++) {
                    const amplitude = Math.max(historyData[i] || 0, 0.01);
                    const barHeight = amplitude * height * 0.9;
                    const x = i * barWidth;
                    const y1 = centerY - barHeight / 2;
                    const y2 = centerY + barHeight / 2;
                    ctx.moveTo(x + barWidth / 2, y1);
                    ctx.lineTo(x + barWidth / 2, y2);
                 }
                 ctx.stroke();
            }

            // --- Timer ---
             function startTimer() {
                if (timerIntervalId) clearInterval(timerIntervalId);
                timerDisplay.textContent = "0:00";
                // Ensure start time is set before interval starts
                if(!recordingStartTime) recordingStartTime = Date.now();
                timerIntervalId = setInterval(() => {
                    // Check if recording is still active before updating timer
                     if (recorderContainer?.dataset.state !== 'recording' || !recordingStartTime) {
                         stopTimer();
                         return;
                     }
                    const elapsedSeconds = Math.floor((Date.now() - recordingStartTime) / 1000);
                    timerDisplay.textContent = formatTime(elapsedSeconds);
                }, 1000);
            }

            function stopTimer() {
                if (timerIntervalId) {
                    clearInterval(timerIntervalId);
                    timerIntervalId = null;
                }
                // Calculate final duration based on when timer was stopped if start time exists
                const finalDuration = recordingStartTime ? Math.floor((Date.now() - recordingStartTime) / 1000) : 0;
               // console.log("Timer stopped. Duration:", finalDuration);
                // Reset start time for next recording
                recordingStartTime = null;
                return finalDuration;
            }

            function formatTime(totalSeconds) {
                const minutes = Math.floor(totalSeconds / 60);
                const seconds = totalSeconds % 60;
                return `${minutes}:${seconds.toString().padStart(2, '0')}`;
            }

            // --- Public Methods / Init ---
            return {
                init: init // Expose only the init function
            };
        })();

        // --- Initialize the component when the DOM is ready ---
        document.addEventListener('DOMContentLoaded', AudioRecorder.init);
    </script>

</body>
</html>